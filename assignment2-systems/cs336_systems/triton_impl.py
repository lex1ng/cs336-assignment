import os
from typing import Callable

import torch
import triton
import triton.language as tl
from einops import rearrange


@triton.jit
def triton_gelu_kernel(x_ptr, y_ptr, num_elements, BLOCK_SIZE: tl.constexpr):

    pid = tl.program_id(axis=0)

    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)

    mask = offsets < num_elements

    x = tl.load(x_ptr + offsets, mask=mask)

    a =  0.79788456 * (x + 0.044715 * x * x * x)
    exp = tl.exp(2 * a)
    tanh = ( exp - 1) / (exp + 1)
    y = 0.5 * x * (1 + tanh)

    tl.store(y_ptr + offsets, y, mask=mask)

@triton.jit
def triton_softmax_kernel(x_ptr, y_ptr, x_row_stride, y_row_stride, num_cols , BLOCK_SIZE: tl.constexpr):

    pid = tl.program_id(0)

    col_offsets = tl.arange(0, BLOCK_SIZE)

    x_start_ptr = x_ptr + pid * x_row_stride
    x_ptrs = x_start_ptr + col_offsets
    x = tl.load(x_ptrs, mask=col_offsets < num_cols, other=float("-inf"))



    norm = x - tl.max(x, 0)
    exp = tl.exp(norm)
    sum_ = tl.sum(exp, axis=0)
    y = exp / sum_

    y_start_ptr = y_ptr + pid * y_row_stride
    y_ptrs = y_start_ptr  + col_offsets
    tl.store(y_ptrs, y, mask=col_offsets < num_cols)

@triton.jit
def weighted_sum_fwd(
    x_ptr, weight_ptr, # 输入和权重向量指针
    output_ptr,        # 输出向量指针
    x_stride_row, x_stride_dim, # x移动步长
    weight_stride_dim, #权重向量移动步长 1
    output_stride_row, #输出向量移动步长 1
    ROWS, D,
    ROW_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,
):
    # x_stride_row = output_stride_row = D, x_stride_dim = 1
    # weight_stride_dim = 1,
    # 知道自己是第几个线程
    # ROWS
    # ROW_TILE_SIZE = 16,
    # D_TILE_SIZE = next_power_of_2(D) // 16
    row_tile_idx = tl.program_id(0)

    # 当前thread的 x输出
    x_block_ptr = tl.make_block_ptr(
        x_ptr,
        shape=(ROWS, D), # 原始x的shape
        strides=(x_stride_row, x_stride_dim),
        offsets=(row_tile_idx * ROW_TILE_SIZE, 0),
        block_shape=(ROW_TILE_SIZE, D_TILE_SIZE),
        order=(1, 0)
    )
    # 当前thread要处理的weight
    weight_block_ptr = tl.make_block_ptr(
        weight_ptr,
        shape=(D, ),
        strides=(weight_stride_dim, 0),
        offsets=(0, ),
        block_shape=(D_TILE_SIZE,),
        order=(0,)
    )
    # 当前thread要处理的输出部分
    output_block_ptr = tl.make_block_ptr(
        output_ptr,
        shape=(D, ),
        strides=(output_stride_row, 0),
        offsets=(row_tile_idx * ROW_TILE_SIZE, 0 ),
        block_shape=(D_TILE_SIZE,),
        order=(0,)
    )
    # 当前thread的结果
    output = tl.zeros((ROW_TILE_SIZE, ), dtype= tl.float32)

    # 当前thread 处理一行数据
    for i in range(tl.cdiv(D, D_TILE_SIZE)):

        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option="zero")
        weight = tl.load(weight_block_ptr, boundary_check=(0), padding_option="zero")

        output += tl.sum(row * weight[None, :], axis=1)

        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))
        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE, ))

    tl.store(output_block_ptr, output, boundary_check=(0, ))


class WeightedSumFunc(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, weight):

        D, output_dims = x.shape[-1], x.shape[:-1]

        input_shape = x.shape

        x = rearrange(x, "... d -> (...) d")

        ctx.save_for_backward(x, weight)



        assert len(weight.shape) == 1 and weight.shape[0] == D, "Dimension mismatch"
        assert x.is_cuda and weight.is_cuda, "Expected CUDA tensors"
        assert x.is_contiguous(), "our pointer arithmetic will assume contiguous x"


        ctx.D_TILE_SIZE = triton.next_power_of_2(D) // 16
        ctx.ROWS_TILE_SIZE = 16
        ctx.input_shape = input_shape

        y = torch.empty(output_dims, device=x.device)



        n_rows = y.numel()

        weighted_sum_fwd[(triton.cdiv(n_rows, ctx.ROWS_TIME_SIZE),)](
            x, weight,
            y,
            x.stride(0), x.stride(1),
            weight.stride(0),
            y.stride(0),
            ROWS=n_rows, D= D,
            ROW_TILE_SIZE=ctx.ROWS_TILE_SIZE, D_TILE_SIZE=ctx.D_TILE_SIZE,
        )

        return y.view(input_shape[:-1])


def print_ptx(name: str, kernel):
    if os.environ.get("TRITON_INTERPRET") == "1":
        # text("PTX is not generated when in interpret mode.")
        return
    """Print out the PTX code generated by Triton for the given `kernel`."""
    ptx_path = f"var/{name}-ptx.txt"
    # Let's go poke around at the PTX code.
    # https://github.com/stanford-cs336/spring2025-lectures/blob/main/var/triton_softmax-ptx.txt
    with open(ptx_path, "w") as f:
        return list(kernel.cache[0].values())[0].asm["ptx"]


def triton_gelu(x):


    assert x.is_cuda == True

    assert x.is_contiguous()

    y = torch.empty_like(x)

    num_elements = x.numel()

    block_size = 1024

    num_blocks = triton.cdiv(num_elements, block_size)

    triton_gelu_kernel[(num_blocks, )](x, y, num_elements, BLOCK_SIZE=block_size)

    return y


def triton_softmax(x):
    assert x.is_cuda == True

    assert x.is_contiguous()

    y = torch.empty_like(x)

    M, N = x.shape
    block_size = triton.next_power_of_2(N)
    num_block = M

    # num_blocks = triton.cdiv(num_elements, block_size)

    triton_softmax_kernel[(num_block, )](x, y, x.stride(0), y.stride(0), N,  BLOCK_SIZE=block_size)
    return y
# def add_kernel(x_ptr, y_ptr, output_ptr, num_elements, BLOCK_SIZE: tl.constexpr):
#
#     pid = tl.program_id(axis=0)
#     start = pid * BLOCK_SIZE
#     offset = start + tl.arange(0, BLOCK_SIZE)
#     mask = offset <= num_elements





